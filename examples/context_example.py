import open_gpt

model = open_gpt.create_model(
    'decapoda-research/llama-7b-hf', precision='fp16', device_map='balanced'
)

context = "The boy's name is Bob."
prompt = "What is the boy's name?"
max_new_tokens = 10


def generate_context(context):
    generated_text = model.step_generate(
        context, max_new_tokens=1, output_past_key_values=True
    )

    for item in generated_text:
        past_key_values = item.pop('past_key_values').past_key_values
        # prefill_output = item['generated_text']

    context_past_key_values = []

    # remove the key_values for last token which is generated by model
    for item in past_key_values:
        tmp = []
        tmp.append(item[0][:, :, :-1, :])
        tmp.append(item[1][:, :, :-1, :])
        context_past_key_values.append(tuple(tmp))
    context_past_key_values = tuple(context_past_key_values)

    return context_past_key_values


context_past_key_values = generate_context(context)

generated_text = model.step_generate(
    prompt,
    max_new_tokens=max_new_tokens,
    output_past_key_values=False,
    past_key_values=context_past_key_values,
)

print(f"===> context: {context}")
print(f"===> prompt: {prompt}")

for idx, item in enumerate(generated_text):
    if idx == max_new_tokens:
        print(item)
